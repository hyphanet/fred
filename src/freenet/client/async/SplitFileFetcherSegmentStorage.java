package freenet.client.async;

import java.io.ByteArrayInputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.lang.ref.SoftReference;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Random;

import freenet.client.FetchException;
import freenet.client.async.PersistentJobRunner.CheckpointLock;
import freenet.crypt.ChecksumFailedException;
import freenet.keys.CHKBlock;
import freenet.keys.CHKDecodeException;
import freenet.keys.CHKEncodeException;
import freenet.keys.CHKVerifyException;
import freenet.keys.ClientCHK;
import freenet.keys.ClientCHKBlock;
import freenet.keys.Key;
import freenet.keys.NodeCHK;
import freenet.node.KeysFetchingLocally;
import freenet.support.Logger;
import freenet.support.MemoryLimitedChunk;
import freenet.support.MemoryLimitedJob;
import freenet.support.io.LockableRandomAccessThing.RAFLock;
import freenet.support.io.NativeThread;
import freenet.support.io.StorageFormatException;

/** Represents a single segment, in memory and on disk. Handles storage and decoding. Note that the
 * on-disk data, and therefore the read-in metadata, may be inaccurate; we check everything 
 * opportunistically. Hence we are very robust (but not completely immune) to disk corruption.
 * @see SplitFileFetcherStorage */
public class SplitFileFetcherSegmentStorage {
    
    // Set this to false to turn off checking the CHKs on blocks decoded (and encoded) via FEC.
    // Generally it is a good idea to have consistent behaviour regardless of what order we fetched
    // the blocks in, whether binary blobs are enabled etc ... and this has caught nasty bugs in
    // the past, although now we have hashes at file level ...
    private static final boolean FORCE_CHECK_FEC_KEYS = true;

    /** The segment number within the splitfile */
    final int segNo;
    /** Offset to the segment's block data. Initially we fill this up. */
    final long segmentBlockDataOffset;
    /** Offset to the segment's status metadata storage. */
    final long segmentStatusOffset;
    /** Length of the segment status for purposes of locating it on disk, may be larger than
     * segmentStatusLength. */
    final int segmentStatusPaddedLength;
    /** Offset to the segment's key list */
    final long segmentKeyListOffset;
    /** Length of the segment key list */
    final int segmentKeyListLength;
    /** The splitfile */
    final SplitFileFetcherStorage parent;
    /** Count of data blocks (actual data divided up into CHKs, though the last one will be 
     * padded). Numbered 0 .. dataBlocks-1. */
    public final int dataBlocks;
    /** Count of cross-segment check blocks. These occur only in larger splitfiles and count as
     * data blocks for segment-level FEC, but they also count as check blocks for cross-segment
     * level FEC. Generally between 0 and 3. Numbered dataBlocks .. dataBlocks+crossSegmentCheckBlocks-1 */
    public final int crossSegmentCheckBlocks;
    /** Count of check blocks (generated by FEC). Numbered 
     * dataBlocks+crossSegmentCheckBlocks .. dataBlocks+crossSegmentCheckBlocks+checkBlocks-1 */
    public final int checkBlocks;
    /** How many times have blocks been retried? Null if maxRetries = -1 */
    private final int[] retries;
    /** Should we write retries to disk? */
    private final boolean writeRetries;
    /** Time at which each block will be sendable. 0 = sendable now. Not serialised - reset when
     * the node is restarted. */
    private final long[] cooldownTimes;
    /** Time at which any block will be sendable. 0 = sendable now. This is only updated when we
     * fail to choose a key to send, and if it increases we tell parent. It is okay for this to be 
     * too small (i.e. we will wake up earlier than expected), but not okay for it to be too large
     * (i.e. we will wake up too LATE). Fortunately all operations here - getting a block, failing 
     * to get a block etc - potentially increase rather than decrease this. */
    private long overallCooldownTime;
    /** Which blocks have we already found? May be inaccurate, checked on FEC decode. */
    private final boolean[] blocksFound;
    /** What is the order of the blocks on disk? Should be kept consistent with blocksFound! Is 
     * read from disk on startup and may be inaccurate, checked on FEC decode. Elements: -1 = not
     * fetched yet. */
    private final short[] blocksFetched;
    private int blocksFetchedCount;
    /** True if we have downloaded and decoded all the data blocks and cross-segment check blocks,
     * and written them to their final location in the parent storage file. */
    private boolean succeeded;
    /** True if we have not only downloaded and decoded, but also finished with encoding and 
     * queueing healing blocks. */
    private boolean finished;
    /** True if the segment has been cancelled, has failed, ran out of retries or otherwise is no
     * longer running but hasn't succeeded. */
    private boolean failed;
    /** True if the metadata needs writing but isn't going to be written immediately. */
    private boolean metadataDirty;
    /** True if the metadata was corrupt and we need to innerDecode(). */
    private boolean corruptMetadata;
    /** The cross segments for each data or cross-segment check block. This allows us to tell the
     * cross-segments when we may have data to decode. The array is null if there are no 
     * cross-segments, and the elements are null if there is no associated cross-segment. */
    private final SplitFileFetcherCrossSegmentStorage[] crossSegmentsByBlock;
    private SoftReference<SplitFileSegmentKeys> keysCache;
    private boolean tryDecode;
    private int crossDataBlocksAllocated;
    private int crossCheckBlocksAllocated;
    /** Number of blocks we've given up on. */
    private int failedBlocks;
    
    private static volatile boolean logMINOR;
    static {
        Logger.registerClass(SplitFileFetcherSegmentStorage.class);
    }
    
    public SplitFileFetcherSegmentStorage(SplitFileFetcherStorage parent, int segNumber, 
            short splitfileType, int dataBlocks, int checkBlocks, int crossCheckBlocks,
            long segmentDataOffset, long segmentKeysOffset, long segmentStatusOffset, 
            boolean writeRetries, SplitFileSegmentKeys keys) {
        this.parent = parent;
        this.segNo = segNumber;
        this.dataBlocks = dataBlocks;
        this.checkBlocks = checkBlocks;
        this.crossSegmentCheckBlocks = crossCheckBlocks;
        int total = dataBlocks + checkBlocks + crossSegmentCheckBlocks;
        this.writeRetries = writeRetries;
        retries = new int[total];
        blocksFound = new boolean[total];
        cooldownTimes = new long[total];
        int minFetched = dataBlocks + crossSegmentCheckBlocks;
        if(crossCheckBlocks != 0)
            crossSegmentsByBlock = new SplitFileFetcherCrossSegmentStorage[minFetched];
        else
            crossSegmentsByBlock = null;
        blocksFetched = new short[minFetched];
        for(int i=0;i<blocksFetched.length;i++) blocksFetched[i] = -1;
        segmentStatusPaddedLength = paddedStoredSegmentStatusLength(dataBlocks, checkBlocks, 
                crossCheckBlocks, writeRetries, parent.checksumLength, parent.persistent);
        segmentKeyListLength = 
            storedKeysLength(dataBlocks, checkBlocks, parent.splitfileSingleCryptoKey != null, parent.checksumLength);
        this.segmentBlockDataOffset = segmentDataOffset;
        this.segmentKeyListOffset = segmentKeysOffset;
        this.segmentStatusOffset = segmentStatusOffset;
        // This must be passed in here or we will read the uninitialised keys!
        keysCache = new SoftReference<SplitFileSegmentKeys>(keys);
    }

    /** Construct from a saved file. Uses the DataInputStream to read static settings, i.e. number 
     * of blocks, does not use the RAF to read block status etc; caller must call readMetadata and
     * readKeys separately for that.
     * @param splitFileFetcherStorage
     * @param raf
     * @param dis DataInputStream to which the static settings have been saved. Anything else we 
     * will need to read separately from the RandomAccessThing.
     * @param segNo The segment number.
     * @throws IOException 
     * @throws StorageFormatException 
     */
    public SplitFileFetcherSegmentStorage(SplitFileFetcherStorage parent, DataInputStream dis, 
            int segNo, boolean writeRetries, long segmentDataOffset, long segmentKeysOffset, 
            long segmentStatusOffset) throws IOException, StorageFormatException {
        this.segNo = segNo;
        this.parent = parent;
        this.dataBlocks = dis.readInt();
        if(dataBlocks < 1 || dataBlocks > 256)
            throw new StorageFormatException("Bad data block count");
        this.crossSegmentCheckBlocks = dis.readInt();
        // REDFLAG one day we will support more than 256 blocks per segment?
        if(crossSegmentCheckBlocks < 0 || crossSegmentCheckBlocks > 256)
            throw new StorageFormatException("Bad cross-segment check block count");
        if(crossSegmentCheckBlocks > 0) 
            throw new StorageFormatException("Cross-segment not supported yet"); // FIXME
        this.checkBlocks = dis.readInt();
        if(checkBlocks < 0 || checkBlocks > 256)
            throw new StorageFormatException("Bad check block count");
        int total = dataBlocks+checkBlocks+crossSegmentCheckBlocks;
        if(total > 256)
            throw new StorageFormatException("Too many blocks in segment");
        // Some of these can be read from the RAF.
        retries = new int[total];
        blocksFound = new boolean[total];
        cooldownTimes = new long[total];
        int minFetched = dataBlocks + crossSegmentCheckBlocks;
        if(crossSegmentCheckBlocks != 0)
            crossSegmentsByBlock = new SplitFileFetcherCrossSegmentStorage[minFetched];
        else
            crossSegmentsByBlock = null;
        blocksFetched = new short[minFetched];
        for(int i=0;i<blocksFetched.length;i++) blocksFetched[i] = -1;
        segmentStatusPaddedLength = paddedStoredSegmentStatusLength(dataBlocks, checkBlocks, 
                crossSegmentCheckBlocks, writeRetries, parent.checksumLength, true);
        segmentKeyListLength = 
            storedKeysLength(dataBlocks, checkBlocks, parent.splitfileSingleCryptoKey != null, parent.checksumLength);
        keysCache = null; // Will be read later
        this.writeRetries = writeRetries;
        this.segmentBlockDataOffset = segmentDataOffset;
        this.segmentKeyListOffset = segmentKeysOffset;
        this.segmentStatusOffset = segmentStatusOffset;
    }

    public SplitFileSegmentKeys getSegmentKeys() throws IOException {
        synchronized(this) {
            if(keysCache != null) {
                SplitFileSegmentKeys cached = keysCache.get();
                if(cached != null) return cached;
            }
            SplitFileSegmentKeys keys;
            try {
                keys = readSegmentKeys();
            } catch (ChecksumFailedException e) {
                Logger.error(this, "Keys corrupted on "+this+" !");
                // Treat as IOException, i.e. fatal. FIXME!
                throw new IOException(e);
            }
            if(keys == null) return keys;
            keysCache = new SoftReference<SplitFileSegmentKeys>(keys);
            return keys;
        }
    }

    SplitFileSegmentKeys readSegmentKeys() throws IOException, ChecksumFailedException {
        SplitFileSegmentKeys keys = new SplitFileSegmentKeys(dataBlocks + crossSegmentCheckBlocks, checkBlocks, parent.splitfileSingleCryptoKey, parent.splitfileSingleCryptoAlgorithm);
        byte[] buf = new byte[SplitFileSegmentKeys.storedKeysLength(dataBlocks, checkBlocks, parent.splitfileSingleCryptoKey != null)];
        parent.preadChecksummed(segmentKeyListOffset, buf, 0, buf.length);
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(buf));
        keys.readKeys(dis, false);
        keys.readKeys(dis, true);
        return keys;
    }
    
    /** Write the status metadata to disk, after a series of updates. */
    public void writeMetadata() throws IOException {
        writeMetadata(true);
    }

    /** Write the status metadata to disk, after a series of updates. */
    public void writeMetadata(boolean force) throws IOException {
        innerWriteMetadata(force);
    }
    
    /** Read all the blocks, encode them according to their supposed keys and check that they are
     * in fact the blocks that they should be. If the metadata is inaccurate, update it and 
     * writeMetadata(). If we have enough blocks to decode, and we don't have all the blocks, then 
     * schedule a decode on the FEC thread. 
     * @return True if we scheduled a decode or are already finished. False if we do not have 
     * enough blocks to decode and need to fetch more blocks. */
    public boolean tryStartDecode() {
        synchronized(this) {
            if(succeeded || failed || finished) return false;
            if(!corruptMetadata && blocksFetchedCount < blocksForDecode()) return false;
            if(tryDecode) return true;
            tryDecode = true;
        }
        long limit = totalBlocks() * CHKBlock.DATA_LENGTH + 
            Math.max(parent.fecCodec.maxMemoryOverheadDecode(dataBlocks + crossSegmentCheckBlocks, checkBlocks),
                    parent.fecCodec.maxMemoryOverheadEncode(dataBlocks + crossSegmentCheckBlocks, checkBlocks));
        final int prio = NativeThread.LOW_PRIORITY;
        parent.memoryLimitedJobRunner.queueJob(new MemoryLimitedJob(limit) {
            
            @Override
            public int getPriority() {
                return prio;
            }
            
            @Override
            public boolean start(MemoryLimitedChunk chunk) {
                CheckpointLock lock = null;
                try {
                    lock = parent.jobRunner.lock();
                    innerDecode(chunk);
                } catch (IOException e) {
                    Logger.error(this, "Failed to decode "+this+" because of disk error: "+e, e);
                    parent.failOnDiskError(e);
                } finally {
                    if(lock != null) lock.unlock(false, prio);
                    chunk.release();
                    synchronized(this) {
                        tryDecode = false;
                    }
                }
                return true;
            }
            
        });
        return true;
    }
    
    /** Attempt FEC decoding. Check blocks before decoding in case there is disk corruption. Check
     * the new decoded blocks afterwards to ensure reproducible behaviour. */
    private void innerDecode(MemoryLimitedChunk chunk) throws IOException {
        if(logMINOR) Logger.minor(this, "Trying to decode "+this+" for "+parent);
        // Even if we fail, once we set tryDecode=true, we need to notify the parent when we're done.
        boolean fail;
        synchronized(this) {
            if(finished) return;
            fail = succeeded || failed;
            if(fail) finished = true;
        }
        if(fail) {
            parent.finishedEncoding(this);
            return;
        }
        
        int totalBlocks = totalBlocks();
        byte[][] allBlocks = readAllBlocks();
        SplitFileSegmentKeys keys = getSegmentKeys();
        if(allBlocks == null || keys == null) {
            return;
        }
        class MyBlock {
            final byte[] buf;
            final short blockNumber;
            final short slot;
            MyBlock(byte[] buf, short blockNumber, short slot) {
                this.buf = buf;
                this.blockNumber = blockNumber;
                this.slot = slot;
            }
        }
        ArrayList<MyBlock> maybeBlocks = new ArrayList<MyBlock>();
        int fetchedCount = 0;
        synchronized(this) {
            boolean[] used = new boolean[totalBlocks];
            for(short i=0;i<blocksFetched.length;i++) {
                if(blocksFetched[i] < 0 || blocksFetched[i] > totalBlocks) {
                    Logger.warning(this, "Inconsistency decoding splitfile: slot "+i+" has bogus block number "+blocksFetched[i]);
                    if(blocksFetched[i] != -1)
                        blocksFetched[i] = -1;
                    maybeBlocks.add(new MyBlock(allBlocks[i], (short)-1, i));
                    continue;
                } else if(used[blocksFetched[i]]) {
                    Logger.warning(this, "Inconsistency decoding splitfile: slot "+i+" has duplicate block number "+blocksFetched[i]);
                    blocksFetched[i] = -1;
                    continue;
                } else {
                    if(logMINOR) Logger.minor(this, "Found block "+blocksFetched[i]+" in slot "+i);
                    maybeBlocks.add(new MyBlock(allBlocks[i], blocksFetched[i], i));
                    used[blocksFetched[i]] = true;
                    fetchedCount++;
                }
            }
            if(fetchedCount < blocksForDecode()) {
                int count = 0;
                for(int i=0;i<totalBlocks;i++) {
                    if(!used[i]) {
                        blocksFound[i] = false;
                    }
                    if(blocksFound[i]) count++;
                }
                if(count != blocksFetchedCount) {
                    Logger.warning(this, "Corrected block count to "+count+" from "+blocksFetchedCount);
                    blocksFetchedCount = count;
                }
            }
        }
        if(fetchedCount < blocksForDecode()) {
            writeMetadata();
            boolean wasCorrupt;
            synchronized(this) {
                wasCorrupt = corruptMetadata;
                corruptMetadata = false;
            }
            parent.restartedAfterDataCorruption(wasCorrupt);
            return;
        }
        
        // Check the blocks and put them into the correct positions.
        int validBlocks = 0;
        int validDataBlocks = 0;
        byte[][] dataBlocks = new byte[this.dataBlocks + this.crossSegmentCheckBlocks][];
        byte[][] checkBlocks = new byte[this.checkBlocks][];
        
        for(MyBlock test : maybeBlocks) {
            boolean failed = false;
            short blockNumber = test.blockNumber;
            byte[] buf = test.buf;
            ClientCHK decodeKey = blockNumber == -1 ? null : keys.getKey(blockNumber, null, false);
            // Encode it to check whether the key is the same.
            try {
                ClientCHKBlock block =
                    ClientCHKBlock.encodeSplitfileBlock(buf, decodeKey.getCryptoKey(), decodeKey.getCryptoAlgorithm());
                ClientCHK actualKey = block.getClientKey();
                if(decodeKey == null || !decodeKey.equals(actualKey)) {
                    // Is it a different block?
                    blockNumber = (short)keys.getBlockNumber(actualKey, null);
                    if(blockNumber == -1) {
                        Logger.error(this, "Block which should be block #"+blockNumber+" for segment "+this+" is not valid for key "+decodeKey);
                        failed = true;
                    } else {
                        synchronized(this) {
                            blocksFetched[test.slot] = blockNumber;
                        }
                    }
                }
                
            } catch (CHKEncodeException e) {
                Logger.error(this, "Block which should be "+blockNumber+" for segment "+this+" cannot be encoded for key "+decodeKey);
                failed = true;
            }
            if(!failed) {
                validBlocks++;
                if(blockNumber < blocksForDecode())
                    validDataBlocks++;
                if(blockNumber < dataBlocks.length)
                    dataBlocks[blockNumber] = buf;
                else
                    checkBlocks[blockNumber - dataBlocks.length] = buf;
            } else {
                synchronized(this) {
                    if(blocksFetched[test.slot] == test.blockNumber) {
                        blocksFetched[test.slot] = (short)-1;
                        blocksFetchedCount--;
                    }
                }
            }
        }
        allBlocks = null;
        maybeBlocks.clear();
        maybeBlocks = null;
        if(validBlocks < blocksForDecode()) {
            writeMetadata();
            boolean wasCorrupt;
            synchronized(this) {
                wasCorrupt = corruptMetadata;
                corruptMetadata = false;
            }
            parent.restartedAfterDataCorruption(wasCorrupt);
            return;
        }
        boolean[] dataBlocksPresent = new boolean[dataBlocks.length];
        boolean[] checkBlocksPresent = new boolean[checkBlocks.length];
        for(int i=0;i<dataBlocks.length;i++) {
            if(dataBlocks[i] == null) {
                dataBlocks[i] = new byte[CHKBlock.DATA_LENGTH];
            } else {
                dataBlocksPresent[i] = true;
            }
        }
        for(int i=0;i<checkBlocks.length;i++) {
            if(checkBlocks[i] == null) {
                checkBlocks[i] = new byte[CHKBlock.DATA_LENGTH];
            } else {
                checkBlocksPresent[i] = true;
            }
        }
        if(validDataBlocks < blocksForDecode()) {
            if(logMINOR) Logger.minor(this, "Decoding in memory for "+this);
            parent.fecCodec.decode(dataBlocks, checkBlocks, dataBlocksPresent, checkBlocksPresent, CHKBlock.DATA_LENGTH);
        }
        boolean capturingBinaryBlob = parent.fetcher.wantBinaryBlob();
        boolean checkDecodedKeys = FORCE_CHECK_FEC_KEYS || capturingBinaryBlob;
        if(checkDecodedKeys) {
            // Check that the decoded blocks correspond to the keys given.
            // This will catch odd bugs and ensure consistent behaviour.
            checkDecodedDataBlocks(dataBlocks, dataBlocksPresent, keys, capturingBinaryBlob);
        }
        writeAllDataBlocks(dataBlocks);
        // Report success if we are not verifying decoded keys, but if we *are*, we need to wait
        // until FEC encoding completes.
        if(!checkDecodedKeys)
            parent.finishedSuccess(this);
        triggerAllCrossSegmentCallbacks();
        parent.fecCodec.encode(dataBlocks, checkBlocks, checkBlocksPresent, CHKBlock.DATA_LENGTH);
        // Check these *after* we complete, to reduce the critical path.
        // FIXME possibility of inconsistency with malicious splitfiles?
        if(checkDecodedKeys) {
            checkEncodedDataBlocks(checkBlocks, checkBlocksPresent, keys, capturingBinaryBlob);
            parent.finishedSuccess(this);
        }
        queueHeal(dataBlocks, checkBlocks, dataBlocksPresent, checkBlocksPresent);
        dataBlocks = null;
        checkBlocks = null;
        writeMetadata();
        // Now we've REALLY finished.
        synchronized(this) {
            corruptMetadata = false;
            finished = true;
        }
        parent.finishedEncoding(this);
    }

    private void checkDecodedDataBlocks(byte[][] dataBlocks, boolean[] dataBlocksPresent, 
            SplitFileSegmentKeys keys, boolean capturingBinaryBlob) {
        for(int i=0;i<dataBlocks.length;i++) {
            if(dataBlocksPresent[i]) continue;
            ClientCHK decodeKey = keys.getKey(i, null, false);
            // Encode it to check whether the key is the same.
            ClientCHKBlock block;
            try {
                block = ClientCHKBlock.encodeSplitfileBlock(dataBlocks[i], decodeKey.getCryptoKey(), decodeKey.getCryptoAlgorithm());
                ClientCHK actualKey = block.getClientKey();
                if(!actualKey.equals(decodeKey)) {
                    if(i == dataBlocks.length-1 && this.segNo == parent.segments.length-1 && 
                            parent.lastBlockMightNotBePadded()) {
                        // Ignore.
                        return;
                    } else {
                        // Usual case.
                        parent.fail(new FetchException(FetchException.SPLITFILE_DECODE_ERROR, "Decoded block does not match expected key"));
                        return;
                    }
                }
                if(capturingBinaryBlob)
                    parent.fetcher.maybeAddToBinaryBlob(block);
            } catch (CHKEncodeException e) {
                // Impossible!
                parent.fail(new FetchException(FetchException.INTERNAL_ERROR, "Decoded block could not be encoded"));
                Logger.error(this, "Impossible: Decoded block could not be encoded");
                return;
            }
        }
    }

    private void checkEncodedDataBlocks(byte[][] checkBlocks, boolean[] checkBlocksPresent, 
            SplitFileSegmentKeys keys, boolean capturingBinaryBlob) {
        for(int i=0;i<checkBlocks.length;i++) {
            if(checkBlocksPresent[i]) continue;
            ClientCHK decodeKey = keys.getKey(i+dataBlocks, null, false);
            // Encode it to check whether the key is the same.
            ClientCHKBlock block;
            try {
                block = ClientCHKBlock.encodeSplitfileBlock(checkBlocks[i], decodeKey.getCryptoKey(), decodeKey.getCryptoAlgorithm());
                ClientCHK actualKey = block.getClientKey();
                if(!actualKey.equals(decodeKey)) {
                    Logger.error(this, "Splitfile check block "+i+" does not encode to expected key for "+this+" for "+parent);
                    return;
                }
                if(capturingBinaryBlob)
                    parent.fetcher.maybeAddToBinaryBlob(block);
            } catch (CHKEncodeException e) {
                // Impossible!
                parent.fail(new FetchException(FetchException.INTERNAL_ERROR, "Decoded block could not be encoded"));
                Logger.error(this, "Impossible: Decoded block could not be encoded");
                return;
            }
        }
    }

    private void queueHeal(byte[][] dataBlocks, byte[][] checkBlocks, boolean[] dataBlocksPresent, boolean[] checkBlocksPresent) throws IOException {
        for(int i=0;i<dataBlocks.length;i++) {
            if(dataBlocksPresent[i]) continue;
            if(retries[i] == 0) continue;
            queueHeal(i, dataBlocks[i]);
        }
        for(int i=0;i<checkBlocks.length;i++) {
            if(checkBlocksPresent[i]) continue;
            if(retries[i+dataBlocks.length] == 0) continue;
            queueHeal(i+dataBlocks.length, checkBlocks[i]);
        }
    }

    private void queueHeal(int blockNumber, byte[] data) throws IOException {
        byte[] cryptoKey;
        byte cryptoAlgorithm;
        if(parent.splitfileSingleCryptoKey != null) {
            cryptoKey = parent.splitfileSingleCryptoKey;
            cryptoAlgorithm = parent.splitfileSingleCryptoAlgorithm;
        } else {
            ClientCHK key = getSegmentKeys().getKey(blockNumber, null, false);
            cryptoKey = key.getCryptoKey();
            cryptoAlgorithm = key.getCryptoAlgorithm();
        }
        parent.fetcher.queueHeal(data, cryptoKey, cryptoAlgorithm);
    }

    private byte[][] readAllBlocks() throws IOException {
        RAFLock lock = parent.lockRAFOpen();
        try {
            // FIXME consider using a single big byte[].
            byte[][] ret = new byte[blocksForDecode()][];
            for(int i=0;i<ret.length;i++)
                ret[i] = readBlock(i);
            return ret;
        } finally {
            lock.unlock();
        }
    }

    private void triggerAllCrossSegmentCallbacks() {
        SplitFileFetcherCrossSegmentStorage[] crossSegmentsByBlockCopy;
        synchronized(this) {
            if(crossSegmentsByBlock == null) return;
            crossSegmentsByBlockCopy = Arrays.copyOf(this.crossSegmentsByBlock, this.crossSegmentsByBlock.length);
            for(int i=0;i<crossSegmentsByBlock.length;i++)
                crossSegmentsByBlock[i] = null;
        }
        for(SplitFileFetcherCrossSegmentStorage s : crossSegmentsByBlockCopy) {
            if(s != null)
                s.onFetchedRelevantBlock(this);
        }
    }

    /** Write a full set of data blocks to disk and update the metadata accordingly. */
    private void writeAllDataBlocks(byte[][] dataBlocks) throws IOException {
        RAFLock lock = parent.lockRAFOpen();
        try {
            synchronized(this) {
                for(int i=0;i<blocksForDecode();i++) {
                    writeDownloadedBlock(i, dataBlocks[i]);
                    blocksFound[i] = true;
                    blocksFetched[i] = (short)i;
                }
                blocksFetchedCount = blocksForDecode();
                succeeded = true;
            }
        } finally {
            lock.unlock();
        }
    }

    final int totalBlocks() {
        return dataBlocks + crossSegmentCheckBlocks + checkBlocks;
    }

    /** A block has been fetched which the caller believes is one of ours. Check whether it is in 
     * fact ours, and that we don't have it already. Find the key and decode it, and add it to our
     * collection. If any cross-segments are waiting for this block, tell them. If we can decode,
     * do so. Can be quite involved, should be called off-thread.
     * @param key
     * @param block
     * @throws IOException If we were unable to write the block to disk.
     * @return True if we successfully decoded a block, in which case the function will be called
     * again. False if there was no match, if we have already fetched that block, or if various 
     * errors occurred.
     */
    public boolean onGotKey(NodeCHK key, CHKBlock block) throws IOException {
        SplitFileSegmentKeys keys = getSegmentKeys();
        if(keys == null) return false;
        short blockNumber;
        ClientCHK decodeKey;
        synchronized(this) {
            if(succeeded || failed || finished) return false;
            blockNumber = (short)keys.getBlockNumber(key, blocksFound);
            if(blockNumber == -1) {
                if(logMINOR) Logger.minor(this, "Block not found "+key);
                return false;
            }
            if(blocksFound[blockNumber]) 
                return false; // Even if this is inaccurate, it will be corrected on a FEC attempt.
            if(blocksFetchedCount >= blocksForDecode())
                return false;
            decodeKey = keys.getKey(blockNumber, null, false);
        }
        ClientCHKBlock decodedBlock;
        byte[] decodedData;
        try {
            decodedBlock = new ClientCHKBlock(block, decodeKey);
            decodedData = decodedBlock.memoryDecode();
        } catch (CHKVerifyException e) {
            Logger.error(this, "Verify failed on block for "+decodeKey);
            return false;
        } catch (CHKDecodeException e) {
            Logger.error(this, "Decode failed on block for "+decodeKey);
            return false;
        }
        if(decodedData.length != CHKBlock.DATA_LENGTH) {
            if(blockNumber == dataBlocks-1 && this.segNo == parent.segments.length-1 && 
                    parent.lastBlockMightNotBePadded()) {
                // Can't use it for FEC decode. Just ignore it.
                // FIXME We can use it if we have all the other data blocks, but it's not worth 
                // checking, and might have non-obvious complications if we e.g. have data loss in
                // FEC decoding.
                Logger.warning(this, "Ignoring last block");
                return false;
            } else {
                parent.fail(new FetchException(FetchException.SPLITFILE_ERROR, "Splitfile block is too short"));
                return false;
            }
        }
        SplitFileFetcherCrossSegmentStorage callback = null;
        // Clearer to do duplicate handling here, plus we only need to decode once.
        do {
            short nextBlockNumber;
            // LOCKING We have to do the write inside the lock to prevent parallel decodes messing up etc.
            synchronized(this) {
                if(succeeded || failed || finished) return true; // We decoded it, it's definitely OK.
                if(blocksFound[blockNumber]) {
                    blockNumber = (short)keys.getBlockNumber(key, blocksFound);
                    continue;
                }
                if(blocksFetchedCount >= blocksForDecode())
                    return true;
                int slotNumber = findFreeSlot();
                assert(slotNumber != -1);
                blocksFetched[slotNumber] = blockNumber;
                blocksFound[blockNumber] = true;
                RAFLock lock = parent.lockRAFOpen();
                try {
                    writeDownloadedBlock(slotNumber, decodedData);
                    innerWriteMetadata(true);
                } catch (IOException e) {
                    blocksFetched[slotNumber] = -1;
                    blocksFound[blockNumber] = false;
                    Logger.error(this, "Unable to write downloaded block to disk: "+e, e);
                    throw e;
                } finally {
                    lock.unlock();
                }
                blocksFetchedCount++;
                if(crossSegmentsByBlock != null && blockNumber < crossSegmentsByBlock.length) {
                    callback = crossSegmentsByBlock[blockNumber];
                    crossSegmentsByBlock[blockNumber] = null;
                }
                nextBlockNumber = (short)keys.getBlockNumber(key, blocksFound);
            }
            if(callback != null)
                callback.onFetchedRelevantBlock(this);
            // Write metadata immediately. Finding a block is a big deal. The OS may cache it anyway.
            writeMetadata();
            if(logMINOR) Logger.minor(this, "Got block "+blockNumber+" ("+key+") for "+this+" for "+parent);
            parent.jobRunner.queueNormalOrDrop(new PersistentJob() {
                
                @Override
                public boolean run(ClientContext context) {
                    parent.fetcher.onFetchedBlock();
                    return false;
                }
                
            });
            tryStartDecode();
            parent.fetcher.maybeAddToBinaryBlob(decodedBlock);
            blockNumber = nextBlockNumber;
        } while(blockNumber != -1);
        return true;
    }

    private synchronized int findFreeSlot() {
        for(int i=0;i<blocksFetched.length;i++) {
            if(blocksFetched[i] == -1) return i;
        }
        return -1;
    }

    /** Caller must have already lock()'ed parent.raf and synchronized(this). 
     * @throws IOException */
    private void writeDownloadedBlock(int slotNumber, byte[] data) throws IOException {
        // FIXME Do we need to pad here for really old splitfiles, or does the FEC code do it?
        if(data.length != CHKBlock.DATA_LENGTH) throw new IllegalArgumentException();
        if(slotNumber >= blocksForDecode()) throw new IllegalArgumentException();
        parent.writeBlock(this, slotNumber, data);
    }

    long blockOffset(int slotNumber) {
        return segmentBlockDataOffset + slotNumber * CHKBlock.DATA_LENGTH;
    }

    /** Write the metadata (status). Caller should already have taken parent.raf.lock() and 
     * synchronized(this). Metadata is fairly sparse on disk, we are expected to deduce it (and
     * check it) when constructing.
     * @throws IOException */
    private void innerWriteMetadata(boolean force) throws IOException {
        if(!parent.persistent) return;
        synchronized(this) {
            if(!(force || metadataDirty)) return;
            OutputStream cos = parent.writeChecksummedTo(segmentStatusOffset, segmentStatusPaddedLength);
            try {
                DataOutputStream dos = new DataOutputStream(cos);
                for(short s : blocksFetched)
                    dos.writeShort(s);
                if(writeRetries) {
                    for(int r : retries)
                        dos.writeInt(r);
                }
                dos.close();
            } catch (IOException e) {
                throw new Error(e); // Impossible!
            }
            metadataDirty = false;
        }
        return;
    }
    
    /** Only called during construction. Reads the variable metadata from the RandomAccessThing. 
     * @throws ChecksumFailedException 
     * @throws StorageFormatException */
    void readMetadata() throws IOException, StorageFormatException, ChecksumFailedException {
        byte[] buf = new byte[segmentStatusPaddedLength];
        try {
            parent.preadChecksummed(segmentStatusOffset, buf, 0, segmentStatusPaddedLength-parent.checksumLength);
        } catch (ChecksumFailedException e) {
            corruptMetadata = true;
            throw e;
        }
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(buf));
        for(int i=0;i<blocksFetched.length;i++) {
            short s = dis.readShort();
            if(s < -1 || s >= retries.length)
                throw new StorageFormatException("Bogus block number in blocksFetched["+i+"]: "+s);
            blocksFetched[i] = s;
            if(s >= 0) {
                if(!blocksFound[s]) {
                    blocksFound[s] = true;
                    blocksFetchedCount++;
                } else {
                    throw new StorageFormatException("Duplicated block number in blocksFetched in "+this);
                }
            }
        }
        int maxRetries = parent.maxRetries;
        if(writeRetries) {
            for(int i=0;i<retries.length;i++) {
                int r = dis.readInt();
                if(r < 0)
                    throw new StorageFormatException("Bogus retry count in retries["+i+"]: "+r);
                retries[i] = r;
                if(maxRetries > -1 && r > maxRetries)
                    failedBlocks++;
            }
        }
        if(failedBlocks >= checkBlocks) {
            failed = true;
        }
        dis.close();
    }

    public static int storedSegmentStatusLength(int dataBlocks, int checkBlocks, int crossCheckBlocks, 
            boolean trackRetries) {
        int fetchedBlocks = dataBlocks + crossCheckBlocks;
        int totalBlocks = dataBlocks + checkBlocks + crossCheckBlocks;
        return fetchedBlocks * 2 + (trackRetries ? (totalBlocks * 4) : 0);
    }
    
    public static int paddedStoredSegmentStatusLength(int dataBlocks, int checkBlocks, int crossCheckBlocks, 
            boolean trackRetries, int checksumLength, boolean persistent) {
        if(!persistent) return 0;
        return storedSegmentStatusLength(dataBlocks, checkBlocks, crossCheckBlocks, trackRetries) +
            checksumLength;
    }
    
    private final int blocksForDecode() {
        return dataBlocks + crossSegmentCheckBlocks;
    }

    public synchronized boolean isFinished() {
        return finished || failed;
    }
    
    public synchronized boolean isDecodingOrFinished() {
        return finished || failed || succeeded || tryDecode;
    }
    
    public synchronized boolean hasSucceeded() {
        return succeeded;
    }

    /** Write content to an OutputStream. We already have raf.lock(). 
     * @throws IOException */
    void writeToInner(OutputStream os) throws IOException {
        // FIXME if we use readAllBlocks() we'll need to run on the memory limited queue???
        for(int i=0;i<dataBlocks;i++) {
            byte[] buf = readBlock(i);
            if(i == dataBlocks-1 && this.segNo == parent.segments.length-1) {
                int length = (int) (parent.finalLength % CHKBlock.DATA_LENGTH);
                if(length == 0) length = CHKBlock.DATA_LENGTH;
                os.write(buf, 0, length);
            } else {
                os.write(buf);
            }
        }
    }

    /** Read a single block from a specific slot, which could be any block number. 
     * @throws IOException If an error occurred reading the data from disk. */
    private byte[] readBlock(int slotNumber) throws IOException {
        if(slotNumber >= blocksForDecode()) throw new IllegalArgumentException();
        return parent.readBlock(this, slotNumber);
    }
    
    public void onNonFatalFailure(int blockNumber) {
        int maxRetries = parent.maxRetries();
        int cooldownTries = parent.cooldownTries;
        long cooldownTime = parent.cooldownLength;
        boolean givenUp = false;
        boolean kill = false;
        boolean wake = false;
        synchronized(this) {
            if(finished || failed || succeeded) return;
            if(blocksFound[blockNumber]) return;
            if(retries[blockNumber]++ == maxRetries) {
                failedBlocks++;
                givenUp = true;
                int target = checkBlocks;
                if(!parent.lastBlockMightNotBePadded()) target++;
                if(failedBlocks >= target) {
                    kill = true;
                    finished = true;
                    failed = true;
                }
            } else {
                if(cooldownTries == 0 || retries[blockNumber] % cooldownTries == 0) {
                    if(logMINOR) Logger.minor(this, "Block "+blockNumber+" entering cooldown on "+this);
                    long now = System.currentTimeMillis();
                    cooldownTimes[blockNumber] = now + cooldownTime;
                    if(overallCooldownTime > cooldownTimes[blockNumber]) {
                        // Will need to wake up earlier than current, so reset the cooldown cache.
                        overallCooldownTime = cooldownTimes[blockNumber];
                        wake = true;
                    }
                } else {
                    if(overallCooldownTime > 0 && overallCooldownTime > System.currentTimeMillis()) {
                        overallCooldownTime = 0;
                        wake = true;
                    }
                }
            }
            if(writeRetries && !kill)
                metadataDirty = true;
        }
        if(writeRetries && !kill)
            lazyWriteMetadata();
        if(givenUp)
            parent.failedBlock();
        if(kill)
            parent.failOnSegment(this);
        if(wake)
            parent.maybeClearCooldown();
    }
    
    /** The metadata has been updated. We should write it ... at some point. CALLER MUST SET metadataDirty! */
    private void lazyWriteMetadata() {
        parent.lazyWriteMetadata();
    }
    
    /** Allocate a cross-segment data block. Note that this algorithm must be reproduced exactly 
     * for splitfile compatibility; the Random seed is actually determined by the splitfile metadata.
     * @param seg The cross-segment to allocate a block for.
     * @param random PRNG seeded from the splitfile metadata, which determines which blocks to 
     * allocate in a deterministic manner.
     * @return The data block number allocated.
     */
    public int allocateCrossDataBlock(SplitFileFetcherCrossSegmentStorage seg, Random random) {
        int size = dataBlocks;
        if(crossDataBlocksAllocated == size) return -1;
        int x = 0;
        for(int i=0;i<10;i++) {
            x = random.nextInt(size);
            if(crossSegmentsByBlock[x] == null) {
                crossSegmentsByBlock[x] = seg;
                crossDataBlocksAllocated++;
                return x;
            }
        }
        for(int i=0;i<size;i++) {
            x++;
            if(x == size) x = 0;
            if(crossSegmentsByBlock[x] == null) {
                crossSegmentsByBlock[x] = seg;
                crossDataBlocksAllocated++;
                return x;
            }
        }
        throw new IllegalStateException("Unable to allocate cross data block even though have not used all slots up???");
    }

    /** Allocate a cross-segment check block. Note that this algorithm must be reproduced exactly 
     * for splitfile compatibility; the Random seed is actually determined by the splitfile metadata.
     * @param seg The cross-segment to allocate a block for.
     * @param random PRNG seeded from the splitfile metadata, which determines which blocks to 
     * allocate in a deterministic manner.
     * @return The block number allocated (between dataBlocks and dataBlocks+crossSegmentCheckBlocks).
     */
    public int allocateCrossCheckBlock(SplitFileFetcherCrossSegmentStorage seg, Random random) {
        if(crossCheckBlocksAllocated == crossSegmentCheckBlocks) return -1;
        int x = dataBlocks + crossSegmentCheckBlocks - random.nextInt(crossSegmentCheckBlocks);
        for(int i=0;i<crossSegmentCheckBlocks;i++) {
            x++;
            if(x == dataBlocks + crossSegmentCheckBlocks) x = dataBlocks;
            if(crossSegmentsByBlock[x] == null) {
                crossSegmentsByBlock[x] = seg;
                crossCheckBlocksAllocated++;
                return x;
            }
        }
        throw new IllegalStateException("Unable to allocate cross check block even though have not used all slots up???");
    }

    static int storedKeysLength(int dataBlocks, int checkBlocks, boolean commonDecryptKey, int checksumLength) {
        return SplitFileSegmentKeys.storedKeysLength(dataBlocks, checkBlocks, commonDecryptKey) + checksumLength;
    }
    
    /** Only called during creation. Do not read the keys in before writing them! */
    void writeKeysWithChecksum(SplitFileSegmentKeys keys) throws IOException {
        assert(keysCache.get() == keys);
        assert(this.dataBlocks + this.crossSegmentCheckBlocks == keys.dataBlocks);
        assert(this.checkBlocks == keys.checkBlocks);
        OutputStream cos = parent.writeChecksummedTo(segmentKeyListOffset, segmentKeyListLength);
        DataOutputStream dos = new DataOutputStream(cos);
        try {
            keys.writeKeys(dos, false);
            keys.writeKeys(dos, true);
        } catch (IOException e) {
            // Impossible!
            throw new Error(e);
        }
        dos.close();
    }

    public boolean definitelyWantKey(NodeCHK key) {
        synchronized(this) {
            if(succeeded || failed || finished) return false;
        }
        SplitFileSegmentKeys keys;
        try {
            keys = getSegmentKeys();
        } catch (IOException e) {
            parent.failOnDiskError(e);
            return false;
        }
        synchronized(this) { // Synched because of blocksFound
            return keys.getBlockNumber(key, blocksFound) >= 0;
        }
    }

    /** Write minimal fixed metadata for the segment. This should include lengths rather than 
     * offsets. Does not write cross-segment block assignments; these are handled by the 
     * cross-segments. 
     * @throws IOException */
    public void writeFixedMetadata(DataOutputStream dos) throws IOException {
        dos.writeInt(this.dataBlocks);
        dos.writeInt(this.crossSegmentCheckBlocks);
        dos.writeInt(this.checkBlocks);
    }

    // For unit testing.
    
    synchronized boolean hasStartedDecode() {
        return succeeded || failed || finished || tryDecode;
    }

    synchronized boolean hasFailed() {
        return failed;
    }

    synchronized boolean[] copyDownloadedBlocks() {
        return this.blocksFound.clone();
    }

    synchronized public long countUnfetchedKeys() {
        if(finished || tryDecode)
            return 0;
        int x = 0;
        for(int i=0;i<blocksFound.length;i++) {
            if(!blocksFound[x]) x++;
        }
        return x;
    }

    synchronized public long countSendableKeys(long now, int maxRetries) {
        if(finished || tryDecode)
            return 0;
        int x = 0;
        for(int i=0;i<blocksFound.length;i++) {
            if(retries[x] >= maxRetries) continue;
            if(cooldownTimes[x] > now) continue;
            if(!blocksFound[x]) x++;
        }
        return x;
    }

    public synchronized void getUnfetchedKeys(List<Key> keys) throws IOException {
        if(finished || tryDecode)
            return;
        SplitFileSegmentKeys keyList = getSegmentKeys();
        for(int i=0;i<blocksFound.length;i++) {
            if(!blocksFound[i])
                keys.add(keyList.getNodeKey(i, null, false));
        }
    }

    /** Pick a key to fetch. Must not update any persistent field. (Cooldowns etc are fine) */
    public int chooseRandomKey(KeysFetchingLocally keysFetching) {
        int[] candidates = new int[blocksFound.length];
        int count = 0;
        int maxRetries = parent.maxRetries();
        long now = System.currentTimeMillis();
        long cooldownTime = Long.MAX_VALUE; // Indicates all blocks have running requests.
        synchronized(this) {
            if(finished) return -1;
            if(tryDecode) {
                if(logMINOR) Logger.minor(this, "Segment decoding so not choosing a key on "+this);
                return -1;
            }
            if(corruptMetadata) return -1; // Will be fetchable after we've found out what blocks we actually have.
            boolean ignoreLastBlock = 
                (this.segNo == parent.segments.length-1 && parent.lastBlockMightNotBePadded());
            SplitFileSegmentKeys keys = null;
            // FIXME OPT try a couple random first? How much does random cost?
            // Find and count all candidates at the smallest retry count.
            // This is O(n), but n <= 256, and memory matters more than clock cycles.
            // Complicated schemes would use more memory as well as creating more bugs,
            // and SoftReference's do have a cost too.
            if(now < overallCooldownTime) return -1; // overallCooldownTime can't decrease.
            long oldOverallCooldownTime = overallCooldownTime;
            int minRetryCount = Integer.MAX_VALUE;
            for(int i=0;i<blocksFound.length;i++) {
                if(blocksFound[i]) continue;
                if(ignoreLastBlock && i == dataBlocks-1) continue;
                int retry = retries[i];
                if(retry > maxRetries && maxRetries != -1) continue;
                if(retry > minRetryCount) continue;
                if(cooldownTimes[i] > now) {
                    cooldownTime = Math.min(cooldownTime, cooldownTimes[i]);
                    continue;
                }
                cooldownTimes[i] = 0;
                if(keys == null) {
                    try {
                        keys = getSegmentKeys();
                    } catch (final IOException e) {
                        parent.jobRunner.queueNormalOrDrop(new PersistentJob() {
                            
                            @Override
                            public boolean run(ClientContext context) {
                                parent.failOnDiskError(e);
                                return true;
                            }
                            
                        });
                        return -1;
                    }
                }
                if(keysFetching.hasKey(keys.getNodeKey(i, null, false), parent.fetcher.getSendableGet()))
                    continue;
                if(retry < minRetryCount) {
                    count = 0;
                    candidates[count++] = i;
                    minRetryCount = retry;
                } else if(retry == minRetryCount) {
                    candidates[count++] = i;
                } // else continue;
            }
            if(count != 0) {
                overallCooldownTime = 0;
                cooldownTime = 0;
            } else {
                overallCooldownTime = cooldownTime;
                if(cooldownTime == oldOverallCooldownTime) cooldownTime = 0; // Don't need to tell parent.
            }
        }
        if(count == 0) {
            if(cooldownTime > now)
                parent.increaseCooldown(this, cooldownTime);
            return -1;
        }
        return candidates[parent.random.nextInt(count)];
    }
    
    public void cancel() {
        boolean decoding;
        synchronized(this) {
            if(finished) return;
            finished = true;
            decoding = tryDecode;
            // If already decoding, must wait for decoder to check in before completing shutdown.
        }
        if(!decoding)
            parent.finishedEncoding(this);
        // Else must wait.
    }

    public synchronized long getOverallCooldownTime() {
        if(finished || succeeded || failed) return 0;
        // Do not recalculate. Calculated in chooseRandomKey(). It's okay if this is smaller than 
        // the true value.
        return overallCooldownTime;
    }

    synchronized long getCooldownTime(int blockNumber) {
        if(finished || succeeded || failed) return 0;
        if(blocksFound[blockNumber]) return 0;
        return cooldownTimes[blockNumber];
    }

    synchronized boolean corruptMetadata() {
        return corruptMetadata;
    }

    public synchronized boolean needsDecode() {
        if(finished || succeeded || failed) return false;
        if(tryDecode) return false;
        return blocksFetchedCount == blocksForDecode();
    }

    public synchronized int foundBlocks() {
        return blocksFetchedCount;
    }

    public synchronized int failedBlocks() {
        return failedBlocks;
    }

}
